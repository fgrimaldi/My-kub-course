# Calico

When it comes to Kubernetes networking, Calico is widely used. One of the main reasons being its ease of use and the way it shapes up the network fabric. Calico is a pure L3 solution, where packets are routed in just the same manner as your regular Internet. Each node (eg. VM) acts like a vRouter, which means tools like traceroute, ping, tcpdump, etc just work as expected! Whether the packet is flowing from one container to another or container to another node (or vice-versa), its just treated as a flat network route (L3 hops). By default, there is no notion of overlays, tunneling or NAT. Each endpoint is actually a /32 IP in IPv4 (or equivalent in other), which means a container can be assigned a public IP. All this is achieved using the Linux kernel’s existing network capabilities. This gives a great flexibility in scaling out the network fabric of a platform running atop Calico.

## Components

Calico is not just limited to programming data routes, but also has rich interface for network policies and now, also supports securing cross-container communications.
The core components of Calico are __Bird__, __Felix__ and a __data-store__ like Etcd, Kubernetes API Server, etc.

- The data-store is used to store the config information(ip-pools, endpoints info, network policies, etc).
- Bird is a per node BGP daemon that exchanges route information with BGP daemons running on other nodes. Common topology could be node-to-node mesh, where each BGP peers with every other. For large scale deployments, this can get messy. To reduce the number of BGP-BGP connections, there are Route Reflectors for completing the route propagation. Certain BGP nodes in that case, can be configured as Route Reflectors.
- Felix is another per-node daemon that is used to configure routes and enforce network policies on the node it is running.
- Other (but equally important) components include Dikastes/Envoy to secure container-to-container communication.

We will be peeking into Calico purely as a standalone CNI plugin, independently of Kubernetes, which I believe will help us in understanding it better.

## Utilities — Calico

Following utilities will be needed.

- __*calico-node*__ (v3.2.3): This is the agent that we will run inside the VMs. It includes Bird, Felix and a few other helper processes.
- __*etcd*__ (v3.3.7): Etcd server (for data-store) will be running on the host. When running on Kubernetes, the preferred way is to use CRDs (via Kube-API Server) as the data-store. One less moving part.
- __*calicoctl*__ (v3.3.0): Client utility to interact with the etcd server to read/write the configuration/status of the cluster.
- __*calico/calico-ipam*__ (v3.2.3): Calico CNI plugins.
- __*cnitool*__ (v0.6.0): Add/remove the containers to the network.

## Setup

My setup includes a multi control plane nodes with 4 worker. This are bare metal server with (ubuntu-18.04. Dedicated Network: 10.10.99.0/24.
Gateway: 10.10.99.1


|HOSTNAME             |ROLES     |VERSION   |INTERNAL-IP|
|---|---|---|---|
|aruba-k8s-master01   |master    |v1.16.3   |10.10.99.101|
|aruba-k8s-master02   |master    |v1.16.3   |10.10.99.102|
|aruba-k8s-master03   |master    |v1.16.3   |10.10.99.103|
|aruba-k8s-worker01   |worker    |v1.16.3   |10.10.99.201|
|aruba-k8s-worker02   |worker    |v1.16.3   |10.10.99.202|
|aruba-k8s-worker03   |worker    |v1.16.3   |10.10.99.203|
|aruba-k8s-worker04   |worker    |v1.16.3   |10.10.99.204|

We can connect directly on `aruba-k8s-master01`

Launch the etcd server on the host and export the environment variable ETCD_ENDPOINTS=https://10.10.99.101:2379.
We are using always the master01, alternatively you can use a Load Balancer.

aruba-k8s-master01:~$ ETCD_ENDPOINTS=https://10.10.99.101:2379

